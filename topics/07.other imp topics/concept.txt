
#Authentication , authorization n admission control:

Security:
    Controlling access to k8s-apiserver is the first line of defence.
    Who can access the cluster - Authentication
    What they can do - Authorization (RBAC)


Kubernetes (K8s) provides robust authentication and authorization mechanisms to secure cluster access and control. These mechanisms are essential for ensuring that only authorized users and processes can interact with the cluster's resources. Here's an overview of authentication and authorization in Kubernetes:

Authentication in Kubernetes:

    Client Certificates: Kubernetes can be configured to use client certificates for authentication. Users and service accounts present their client certificates when interacting with the API server. These certificates are then validated against the cluster's Certificate Authority (CA).

    Bearer Tokens: Users can authenticate using bearer tokens. Tokens are generated by the cluster's Token Controller and are associated with specific service accounts or users. Users present their tokens in the Authorization header of API requests.

    Service Account Tokens: Pods running in the cluster can use service account tokens to authenticate with the Kubernetes API. These tokens are automatically mounted into the pod's filesystem, allowing the pod to make authenticated API calls.

    OpenID Connect (OIDC): Kubernetes can be integrated with OIDC identity providers. This allows users to log in using their existing OIDC identities, such as Google or Azure Active Directory.

    Static Password Files: While not recommended for production, Kubernetes can use static password files for authentication. This method is typically used in development or testing environments.

Authorization in Kubernetes:

    Role-Based Access Control (RBAC): RBAC is the most commonly used authorization mechanism in Kubernetes. It allows cluster administrators to define roles and role bindings that specify which users, groups, or service accounts have access to specific resources and what actions they can perform. RBAC is highly configurable and granular.

    Node Authorization (NodeAuthz): Node authorization controls whether a kubelet running on a node is authorized to modify the node's API objects. It uses a Node Authorizer, which is separate from RBAC.

    Attribute-Based Access Control (ABAC): ABAC allows administrators to define policies using attributes associated with API requests. It's less commonly used than RBAC and is typically not recommended for complex scenarios.

    Webhook Authorization: Kubernetes supports webhook-based authorization, allowing external systems to make authorization decisions based on custom logic. This can be useful for integrating with external identity and access management systems.

    Admission Controllers: Admission controllers can be used to enforce custom policies and security checks during resource creation and updates. While they are not a direct form of authorization, they can complement RBAC and other authorization mechanisms.

kubeconfig:
    A Kubeconfig file, often referred to as the Kubernetes configuration file, is a YAML file used to configure access to a Kubernetes cluster. It contains information about one or more clusters, the associated authentication credentials, and other configuration details that allow users and applications to interact with the cluster. Kubeconfig files are crucial for managing access and context when working with Kubernetes clusters.

    Here's an explanation of the key components and sections found in a Kubeconfig file:

    Clusters:
        clusters section defines the Kubernetes clusters you can access.
        Each cluster entry typically includes:
        name: A user-defined name for the cluster.
        cluster:
        server: The URL or hostname of the Kubernetes API server.
        certificate-authority-data or certificate-authority: The root certificate authority (CA) used to authenticate the API server's certificate (or a path to the CA file).
        insecure-skip-tls-verify (optional): If set to true, TLS certificate validation is skipped (not recommended for production).
    
    Users:
        users section defines the authentication credentials for users or service accounts.
        Each user entry typically includes:
        name: A user-defined name for the user.
        user: Authentication information. Depending on the authentication method, this can include:
        client-certificate-data or client-certificate: The user's client certificate (or a path to the certificate file).
        client-key-data or client-key: The user's private key (or a path to the key file).
        token: A bearer token used for authentication.
        username and password: Username and password for basic authentication (not recommended for production).
    
    Contexts:
        contexts section combines clusters and users to define specific contexts for interacting with a cluster.
        Each context entry typically includes:
        name: A user-defined name for the context.
        context:
        cluster: The cluster to use (refers to a cluster entry).
        user: The user or service account to use (refers to a user entry).
        namespace (optional): The default namespace for this context.
    
    Current Context:
        current-context specifies the context that should be used by default when issuing kubectl commands. It refers to one of the context entries defined in the contexts section.
   
    Multi-Cluster Configurations:
        Kubeconfig files can include multiple cluster, user, and context entries, allowing users to switch between different clusters and authentication methods easily.
    
    Cluster-specific Configuration:
        You can have different Kubeconfig files for different clusters, making it easy to manage access to multiple clusters.
    
    Location:
        By default, Kubeconfig files are located at ~/.kube/config on your local machine. However, you can specify a different location using the KUBECONFIG environment variable.
        You can specify the Kubeconfig file location using the --kubeconfig flag with kubectl. For example, kubectl --kubeconfig=/path/to/config.yaml get pods.
    
    Kubeconfig files can include other configuration options, such as context-specific settings or cluster-specific settings, depending on your requirements.
   
    Kubeconfig files are used primarily with the kubectl command-line tool, which reads the configuration to determine which cluster to connect to, which user or service account 
    to authenticate as, and which namespace to operate in. By managing multiple contexts within the Kubeconfig file, users can switch between different clusters and environments seamlessly.


API Groups:
    The REST API is the fundamental fabric of Kubernetes. All operations and communications between components, and external user commands are REST API calls that the API Server handles. 
    Consequently, everything in the Kubernetes platform is treated as an API object and has a corresponding entry in the API.
 
    API groups are a fundamental concept that helps organize and categorize the various resources and objects that the Kubernetes API manages. They provide a way to group related resources
    under a common API endpoint, making it easier to extend and manage the Kubernetes API.

    Mainly two api Groups:
    - Core Group:
        The core API group, often referred to as the "legacy" or "v1" group, includes resources that are considered fundamental to Kubernetes, such as Pods, Services, and ConfigMaps. 
        These resources are available under the /api/v1 endpoint.
    - Named Group:
        The named groups are at REST path /apis/$GROUP_NAME/$VERSION and use apiVersion: $GROUP_NAME/$VERSION (for example, apiVersion: batch/v1).

    In Kubernetes, resources and objects are identified using a combination of API group, API version, and resource name. The structure of an API endpoint typically follows this 
    pattern: /apis/{group}/{version}/{resource}.

    Each API group can have multiple API versions, which represent different iterations or changes to the resources within that group. As Kubernetes evolves, new API versions are introduced, 
    and older versions may be deprecated over time.
    There is a plugin called kubectl-convert, to convert old api version yaml file to new api version yaml file.

    Kubernetes typically follows a strategy where new features and resources are introduced in alpha or beta API versions, and stable versions are later promoted. This helps ensure that new 
    features can be tested and stabilized before they become the default.

    Access control and RBAC (Role-Based Access Control) policies can be applied at the API group level. This means that you can define permissions and roles specific to a particular API group 
    or even for individual custom resources within that group.

    Examples:
        The "apps" API group includes resources like Deployments and StatefulSets (/apis/apps/v1).
        The "batch" API group includes resources like CronJobs (/apis/batch/v1).
        Custom resources can be created under user-defined API groups, such as "mycompany.com" (/apis/mycompany.com/v1).

RBAC(Authentication mechanism):
    Role-Based Access Control (RBAC) is a fundamental security mechanism in Kubernetes that regulates access to the resources and operations within a cluster. RBAC allows administrators to define 
    fine-grained permissions, determining who can perform specific actions on which resources in the cluster. 
    It is a crucial component for securing Kubernetes clusters and ensuring that only authorized users and applications can interact with them.

    Here are the key concepts and components of RBAC in Kubernetes:
        Roles: A Role is a set of rules that defines a set of permissions within a namespace. Roles are specific to a namespace and can only be used to grant permissions within that namespace. They are typically used to define what actions (e.g., create, read, update, delete) are allowed on specific resources (e.g., Pods, ConfigMaps) within a namespace.

        ClusterRoles: Similar to Roles, ClusterRoles are sets of rules that define permissions, but they are not limited to a single namespace. ClusterRoles are cluster-wide and can be used to grant permissions across all namespaces. They are often used for cluster-level actions, such as managing nodes or custom resources.

        RoleBindings: A RoleBinding associates a Role or ClusterRole with one or more subjects (e.g., users, service accounts, groups). It specifies which subjects have which roles and in which namespaces. RoleBindings are used to bind RBAC policies to users and service accounts.

        ClusterRoleBindings: ClusterRoleBindings are similar to RoleBindings but operate at the cluster level. They bind ClusterRoles to subjects across all namespaces in the cluster. ClusterRoleBindings are used for cluster-wide RBAC policies.

        Subjects: Subjects are entities that can be granted access to resources. Subjects can be users, groups, or service accounts. Kubernetes authenticates subjects using various mechanisms (e.g., client certificates, bearer tokens) before applying RBAC policies.

        Verbs: Verbs are actions that can be performed on resources, such as "get," "list," "create," "update," and "delete." RBAC rules specify which verbs are allowed or denied for specific resources.

    RBAC in Kubernetes allows administrators to control access at a granular level, helping to enforce the principle of least privilege and enhance the security of Kubernetes clusters. 
    It is a critical component for managing cluster security and access control.


Admission Controllers:
    Flow of user request to apiserver:
    User ===========> Authentication =========> Authorization ==============> Admission controllers ======> Pod created

    Example:
    When a user tries to create a pod in non-existing namespace, the request is rejected. Namespace Lifecycle of Adminssion
    controller is the reason for rejection.

    Kubernetes admission controllers are plugins that govern and enforce how the cluster is used. They can be thought of as a gatekeeper that intercept (authenticated) API requests and may change the request object or deny the request altogether. The admission control process has two phases:
    the mutating phase is executed first, followed by the validating phase.

    Admission controllers are divided into two basic classes for ease of use.
        Validating admission controllers: These types of admission controllers validate API object contents and make sure they are valid.
         The responses are returned as binary results in the form of yes, if the object is granted permission, or no when it is not.
        Mutating admission controllers: These admission controllers assess the API object and may add or modify the API object contents.

    It’s also worth noting that admission controllers can be both validating and mutating controllers. A great example of that is the LimitRanger admission controller, which can handle functioning for both of these controller methods. As a validating admission controller, it can allow 
    or deny the deployment objects from violating the limit range constraints. Meanwhile, as a mutating admission controller, it can change or assign resource limits for pods.

    Kubernetes recommends enabling several built-in admission controllers by default to secure the running containers:
        PodSecurityPolicy: This controller implements pod admission (i.e., create and modify pods) and determines whether they should be admitted based on the security policies. Note that PSPs will be deprecated in a future Kubernetes release.
                
        AlwaysPullImages: This controller ensures that the latest builds for the container images are always downloaded. While there is a performance deficit for pulling and using new images every time for a node, it’s important to run up-to-date container images.
        
        LimitRange and ResourceQuota: This controller observes incoming requests and checks them for limits violations to prevent DoS attacks and unauthorized processes from running.
        
        DefaultStorageClass: This controller handles PersistentVolumeClaim (PVC) objects creation. It also automatically adds a default storage class to them if they are not mapped to any specific ones.
    
        NamespaceLifecycle: This controller ensures that namespaces under the process of termination are not allowed to create new objects. All the requests made to nonexisting namespaces are rejected, while modification of system namespaces is disabled.
    
        Service Account: In Kubernetes, service accounts are processes that run in pods. This controller implements the automation of service accounts.
                
        MutatingAdmissionWebhook and ValidatingAdmissionWebhook: These are dynamic admission controllers that can be extended with custom logic through webhooks.

    In Kubernetes, dynamic admission controllers are user-configured controllers that can modify or reject API requests based on custom logic.

    Open Policy Agent (OPA) as a Dynamic Admission Controller. Developed by Styra, OPA is an open-source framework used to enforce policy decisions on clusters, CI/CD pipelines, and microservices. It’s currently a part of the Cloud Native Computing Foundation (CNCF) as an incubating project. 
    OPA can be tightly integrated with Kubernetes to make policy decisions.
    Kubernetes specific implementation of OPA happens through an OPA Gatekeeper agent, which acts as a validating webhook to enforce the policies. Gatekeeper only checks whether the request meets the defined criteria or not, which is why it is installed as a validating webhook.

Pod security policies:
    PSP removed in v1.25. instead use Pod Security Adminssion.

Pod disruption budget:
    A PodDisruptionBudget (PDB) is a resource in Kubernetes that allows you to define and control the disruption or unavailability of Pods during certain events, such as maintenance, scaling down, or node draining. PDBs are used to ensure the availability and reliability of applications in your 
    cluster by setting constraints on how many Pods of a particular application can be disrupted simultaneously.

    Use Cases:
        Maintenance: PDBs are commonly used when performing cluster maintenance, such as node upgrades or hardware maintenance. They ensure that the impact on application availability is controlled.
        Scaling Down: When scaling down a deployment or a StatefulSet, a PDB can ensure that the old Pods are not terminated too quickly, allowing for a smooth transition.
        Node Drain: During node draining, when a node is decommissioned or drained for maintenance or scaling down, PDBs help maintain application availability by controlling the rate at which Pods are moved to other nodes.

    The PodDisruptionBudget ensures that during voluntary disruptions, such as scaling down the Deployment or draining nodes, no more than one Pod is disrupted at a time, maintaining the desired level of availability.

    Please note that while PodDisruptionBudgets are useful for planned disruptions, they do not protect against unplanned disruptions, such as node failures. For handling unplanned disruptions, you might also consider
    features like anti-affinity rules or ReplicaSets with higher replicas.

#Custom Resource Definition

    CRDs extends the k8s API. If we want to create our own objects just like native objects like deployment, pod,...,we create a crd first and then 
    create our cutom object(cutom resource).

    Now that we have created a custom object and this info is stored in etcd db, this will not do anything.
    example:
        Flight booking object created. It will not actually book ticket. Object has to call Flight booking API to book the ticket.    
        Inorder to watch out for any changes top custom object, we have to built our own cutom controller preferrably in Go language.

    Custom Controller:
        A controller is any process or code that runs in a loop and is continuously monitoring the k8s cluster and listening to events of specific objects being changed.
        
        The custom controllers manage the lifecycle of applications based on the CRs, automating tasks like provisioning, scaling, backup, and recovery.

Operator Framework:
    CRDs and custom controllers are seprate entities. we have to create the crds manually and then the resources and then deploy the controller
    as a pod or deployment. However these two entities can be packaged together to deploy as a single entity using the operator framework.

    Operators are a way to automate the management of complex applications and services in Kubernetes by extending the Kubernetes API with custom resources and controllers.

    Let's walk through how an end user interacts with the NginxWebApp operator using an example. In this scenario, an end user wants to deploy and manage Nginx web servers using the operator.

    Discover the Operator:
        The end user first discovers the NginxWebApp operator, which has been previously installed in the Kubernetes cluster. They may find it in a catalog like the Operator Hub or through documentation provided by the cluster administrator.

    Create a Custom Resource:
        To deploy Nginx web servers, the end user creates a custom resource (CR) of kind NginxWebApp. They can define the desired configuration for their Nginx deployment in the CR. For example, they may want to deploy three replicas of Nginx:

    # nginxwebapp-sample.yaml

    apiVersion: app.example.com/v1
    kind: NginxWebApp
    metadata:
    name: my-nginx
    spec:
    replicas: 3
    The end user can save this configuration in a YAML file, such as nginxwebapp-sample.yaml.

    Apply the Custom Resource:
        The end user applies the custom resource to the Kubernetes cluster using the kubectl apply command:

    kubectl apply -f nginxwebapp-sample.yaml

    This informs the operator about the desired state—three Nginx replicas.

    Operator Reconciliation:
        Behind the scenes, the NginxWebApp operator's custom controller watches for changes to custom resources of kind NginxWebApp. When the end user applies the custom resource, the operator's controller detects it and starts reconciling the desired state.

    Operator Actions:
        The operator's reconciliation logic takes action based on the desired state. In this case, it creates and manages Nginx deployments to ensure there are three replicas running in the cluster.

    Monitoring the Application:
        The end user can monitor the progress and status of the Nginx deployment by checking the custom resource's status field. The operator may update the status field to indicate that the desired state has been achieved:

    kubectl get nginxwebapp my-nginx -o=jsonpath='{.status}'

    Updating the Configuration:
        If the end user wants to make changes to the Nginx deployment, such as increasing the number of replicas to five, they simply update the nginxwebapp-sample.yaml file and reapply it using kubectl apply.

    # Updated nginxwebapp-sample.yaml

    apiVersion: app.example.com/v1
    kind: NginxWebApp
    metadata:
    name: my-nginx
    spec:
    replicas: 5

    kubectl apply -f nginxwebapp-sample.yaml

    The operator will detect the change and adjust the deployment accordingly.

    Deletion and Cleanup:
        When the end user no longer needs the Nginx deployment, they can simply delete the custom resource:

    kubectl delete nginxwebapp my-nginx

    The operator will respond by cleaning up the Nginx deployment.

    In this way, the NginxWebApp operator abstracts the complexities of deploying and managing Nginx web servers in Kubernetes. The end user interacts with the operator by defining their desired state using custom resources, 
    and the operator takes care of the underlying actions required to achieve that state. This approach simplifies application management and ensures consistency in the cluster.

